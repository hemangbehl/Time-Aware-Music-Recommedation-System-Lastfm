{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#from pyspark.ml.recommendation import ALS\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = SparkSession.builder.appName(\"s\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot1 = sc.textFile(r\"F:\\Data_Repository\\lastfm\\df_slot1.tsv\")\n",
    "slot2 = sc.textFile(r\"F:\\Data_Repository\\lastfm\\df_slot2.tsv\")\n",
    "slot3 = sc.textFile(r\"F:\\Data_Repository\\lastfm\\df_slot3.tsv\")\n",
    "slot4 = sc.textFile(r\"F:\\Data_Repository\\lastfm\\df_slot4.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(slot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rates_and_preds = data2.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199, 1107), (1.0, 0.9714728755995674))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###temp\n",
    "#rates_and_preds.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = [slot1, slot2, slot3, slot4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    return False\n",
    "\n",
    "def computeRMSE(model,data):\n",
    "    \n",
    "    \"\"\" Takes ALS models and testing data as input and returns RMSE value \"\"\"\n",
    "    \n",
    "    data_for_predict = data.map(lambda x: (x[0], x[1]))\n",
    "    \n",
    "    predictions = model.predictAll(data_for_predict).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 5 the RMSE is 1.1681614305385888\n",
      "For rank 10 the RMSE is 1.1678819944463574\n",
      "For rank 15 the RMSE is 1.1671706920636706\n",
      "The best model was trained with rank 15\n",
      "Saving model to the given path:\n",
      "For rank 5 the RMSE is 1.0204170683254816\n",
      "For rank 10 the RMSE is 1.020022155681369\n",
      "For rank 15 the RMSE is 1.0196947585522675\n",
      "The best model was trained with rank 15\n",
      "Saving model to the given path:\n",
      "For rank 5 the RMSE is 1.2072978578503877\n",
      "For rank 10 the RMSE is 1.2066129841598738\n",
      "For rank 15 the RMSE is 1.2064489655607\n",
      "The best model was trained with rank 15\n",
      "Saving model to the given path:\n",
      "For rank 5 the RMSE is 1.1906307046531146\n",
      "For rank 10 the RMSE is 1.189835280967131\n",
      "For rank 15 the RMSE is 1.1895705703178825\n",
      "The best model was trained with rank 15\n",
      "Saving model to the given path:\n",
      "Wall time: 13min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelnameid = 1\n",
    "for data in slots:\n",
    "    path = str('F:\\Data_Repository\\lastfm')\n",
    "    modelname = path + \"\\slot\" + str(modelnameid) + \".tsv\"\n",
    "    data = data.map(lambda x: x.split('\\t'))\n",
    "    data2 = data.map(lambda x : [x[i] for i in [0,1,2]]) #only 3 columns exist\n",
    "    data2 = data2.filter(lambda x: isNumber(x[2])) # Remove faulty rows\n",
    "    data2 = data2.map(lambda x: [x[0], x[1], float(x[2])]) #Change plays into float\n",
    "    users = data2.map(lambda x: x[0]).distinct().zipWithIndex()\n",
    "    artists = data2.map(lambda x: x[1]).distinct().zipWithIndex()\n",
    "    data2 = data2.map(lambda r: (r[0], (r[1], r[2]))).join(users).map(lambda r: (r[1][1], r[1][0][0], r[1][0][1]))\n",
    "    data2 = data2.map(lambda r: (r[1], (r[0], r[2]))).join(artists).map(lambda r: (r[1][0][0], r[1][1], r[1][0][1]))\n",
    "    plays = data2.map(lambda x: x[2])\n",
    "    data2 = data2.map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "    training_RDD, validation_RDD, test_RDD = data2.randomSplit([6, 2, 2], seed = 2)\n",
    "    validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "    test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "    \n",
    "    seed = 5\n",
    "    #iterations = 10\n",
    "    iterations = 1\n",
    "    regularization_parameter = 0.1\n",
    "    ranks = [5,10,15]\n",
    "    #ranks = [15] #to reduce loop\n",
    "    errors = [0, 0, 0]\n",
    "    err = 0\n",
    "    tolerance = 0.02\n",
    "    alpha = 0.01\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_iteration = -1\n",
    "\n",
    "    for rank in ranks:\n",
    "        e = 0\n",
    "        #for i in range(5):\n",
    "        for i in range(1):\n",
    "            # Split the data\n",
    "            training_RDD, validation_RDD, test_RDD = data2.randomSplit([6, 2, 2], seed = 2)\n",
    "            validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "            test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "            model = ALS.trainImplicit(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                              lambda_=regularization_parameter,alpha=alpha)\n",
    "        #     predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "        #     rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "        #     error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "            e += computeRMSE(model,validation_RDD)\n",
    "\n",
    "        error = e/5.5\n",
    "        errors[err] = error\n",
    "        err += 1\n",
    "        print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_rank = rank\n",
    "\n",
    "    print ('The best model was trained with rank %s' % best_rank)\n",
    "\n",
    "    ####to save model\n",
    "    print(\"Saving model to the given path:\")\n",
    "    model.save(sc, modelname)\n",
    "    modelnameid = modelnameid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Data_Repository\\lastfm\\slot1.tsv\n",
      "F:\\Data_Repository\\lastfm\\slot2.tsv\n",
      "F:\\Data_Repository\\lastfm\\slot3.tsv\n",
      "F:\\Data_Repository\\lastfm\\slot4.tsv\n"
     ]
    }
   ],
   "source": [
    "# modelnameid = 1\n",
    "# for slot in range(0,4):\n",
    "#     path = str('F:\\Data_Repository\\lastfm')\n",
    "#     modelname = path + \"\\slot\" + str(modelnameid) + \".tsv\"\n",
    "#     print(modelname)\n",
    "#     modelnameid = modelnameid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: x.split('\\t'))\n",
    "# header = data.first()\n",
    "# print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2 = data.map(lambda x : [x[i] for i in [0,1,3]])\n",
    "data2 = data.map(lambda x : [x[i] for i in [0,1,2]]) #only 3 columns exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"length of uncleaned data -\",data2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_000001', '15 Step', '2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating(user=1, product=1107, rating=10.0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.filter(lambda x: isNumber(x[2])) # Remove faulty rows\n",
    "data2 = data2.map(lambda x: [x[0], x[1], float(x[2])]) #Change plays into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_000001', '15 Step', 2.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no need to reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of slot data - 247852\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Filter out values with more than 500 plays (for this sake of simplicity)\n",
    "#print (\"length of cleaned data -\",data2.count())\n",
    "#data2 = data2.filter(lambda x : x[2] <= 50)\n",
    "# print (data2.take(2))\n",
    "print (\"length of slot data -\",data2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Convert strings into integers\n",
    "users = data2.map(lambda x: x[0]).distinct().zipWithIndex()\n",
    "artists = data2.map(lambda x: x[1]).distinct().zipWithIndex()\n",
    "# int_user = users.map(lambda u: (u[1], u[0]))\n",
    "# int_artist = artists.map(lambda i: (i[1], i[0]))\n",
    "# users.collect()\n",
    "# artists.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Substitutes the ObjectIDs in the ratings RDD with the corresponding int values\n",
    "data2 = data2.map(lambda r: (r[0], (r[1], r[2]))).join(users).map(lambda r: (r[1][1], r[1][0][0], r[1][0][1]))\n",
    "data2 = data2.map(lambda r: (r[1], (r[0], r[2]))).join(artists).map(lambda r: (r[1][0][0], r[1][1], r[1][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data2.filter(lambda x: x[0] == 12).collect()\n",
    "# plays = data2.map(lambda x: x[2]).collect() ##seems like data2 loses data after .collect is called\n",
    "plays = data2.map(lambda x: x[2])\n",
    "# data2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 931, 11.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use 'Rating' function to get the values in the right format\n",
    "data2 = data2.map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "#data2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe converted to rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating(user=1, product=931, rating=11.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use randomsplit to split the data into train, validation and testing sets\n",
    "\n",
    "training_RDD, validation_RDD, test_RDD = data2.randomSplit([6, 2, 2], seed = 2)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define computeRMSE\n",
    "\n",
    "def computeRMSE(model,data):\n",
    "    \n",
    "    \"\"\" Takes ALS models and testing data as input and returns RMSE value \"\"\"\n",
    "    \n",
    "    data_for_predict = data.map(lambda x: (x[0], x[1]))\n",
    "    \n",
    "    predictions = model.predictAll(data_for_predict).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 5 the RMSE is 1.2849775735924476\n",
      "For rank 10 the RMSE is 1.284670193890993\n",
      "For rank 15 the RMSE is 1.2838877612700377\n",
      "The best model was trained with rank 15\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed = 5\n",
    "#iterations = 10\n",
    "iterations = 1\n",
    "regularization_parameter = 0.1\n",
    "ranks = [5,10,15]\n",
    "#ranks = [15] #to reduce loop\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "alpha = 0.01\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    e = 0\n",
    "    #for i in range(5):\n",
    "    for i in range(1):\n",
    "        # Split the data\n",
    "        training_RDD, validation_RDD, test_RDD = data2.randomSplit([6, 2, 2], seed = 2)\n",
    "        validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "        test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "        \n",
    "        model = ALS.trainImplicit(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                          lambda_=regularization_parameter,alpha=alpha)\n",
    "    #     predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    #     rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    #     error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "        e += computeRMSE(model,validation_RDD)\n",
    "    \n",
    "    error = e/5\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % best_rank)\n",
    "# For rank 5 the RMSE is 6.67535527624558\n",
    "# For rank 10 the RMSE is 6.461547082828238\n",
    "# For rank 15 the RMSE is 6.531429770101694\n",
    "# The best model was trained with rank 10\n",
    "# Wall time: 8min 21s\n",
    "\n",
    "# For rank 5 the RMSE is 1.2219171452924378\n",
    "# For rank 10 the RMSE is 1.5340979416430192\n",
    "# For rank 15 the RMSE is 1.2701273441785674\n",
    "# The best model was trained with rank 5\n",
    "# Wall time: 1min 33s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Final Model\n",
    "\n",
    "# model = ALS.train(training_RDD, best_rank, seed=seed, iterations= iterations,\n",
    "#                       lambda_=regularization_parameter)\n",
    "# predictions = model.predictAll(validation_for_predict_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final Model\n",
    "\n",
    "model = ALS.train(data2, best_rank, seed=seed, iterations= iterations,\n",
    "                      lambda_ = regularization_parameter)\n",
    "#predictions = model.predictAll(validation_for_predict_RDD)\n",
    "predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.093980964206101"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#     rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "#     error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "computeRMSE(model,validation_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((819, 41), (1.0, 1.3774459478762293))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating(user=23, product=931, rating=4.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## validation rdd\n",
    "validation_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 931)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## validation having just user item\n",
    "validation_for_predict_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predictAll(validation_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49731\n",
      "49731\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print (validation_RDD.count())\n",
    "# print (predictions.count()   )\n",
    "# #computeRMSE(model,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#computeRMSE(model,data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend Products for top-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendProducts(self, user, num):\n",
    "    \"\"\"\n",
    "    Recommends the top \"num\" number of products for a given user and\n",
    "    returns a list of Rating objects sorted by the predicted rating in\n",
    "    descending order. \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "n = 2\n",
    "recos = model.recommendProducts(0, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=2679, rating=0.04412376419356484),\n",
       " Rating(user=0, product=841, rating=0.04395402252539503)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for i in recos:\n",
    "     scores.append(float(i[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04412376419356484, 0.04395402252539503]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what doe this do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_for_users = model.recommendProductsForUsers(1).collect()\n",
    "# len(products_for_users)\n",
    "# #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products_for_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\"::\")\n",
    "    return Rating(int(fields[0]), int(fields[1]), float(fields[2]) - 2.5)\n",
    "#ratings = data2.map(lambda r: parseLine(r)) #already in rating format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a model on to predict user-product ratings\n",
    "model = ALS.train(data2, 15, 2, 0.01)\n",
    "ratings = data2\n",
    "# Get predicted ratings on all existing user-product pairs\n",
    "testData = ratings.map(lambda p: (p.user, p.product))\n",
    "predictions = model.predictAll(testData).map(lambda r: ((r.user, r.product), r.rating))\n",
    "\n",
    "ratingsTuple = ratings.map(lambda r: ((r.user, r.product), r.rating))   #\n",
    "scoreAndLabels = predictions.join(ratingsTuple).map(lambda tup: tup[1]) # an RDD of (prediction, observation) pairs.\n",
    "\n",
    "# Instantiate regression metrics to compare predicted and actual ratings\n",
    "metrics = RegressionMetrics(scoreAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 3.4324853414348473\n"
     ]
    }
   ],
   "source": [
    "# Root mean squared error\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scoreAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankingmetrics = RankingMetrics(scoreAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels = testData.map(lambda lp: (int(model.predict(lp.features)), lp.label))\n",
    "predictionAndLabels = data2.map(lambda r: ( float(r[2]) ) ).join(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictionAndLabels – an RDD of (prediction, label) pairs.\n",
    "\n",
    "predictionAndLabels – an RDD of (predicted ranking, ground truth set) pairs.\n",
    "\n",
    "metrics = RankingMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val prediction: RDD[Rating] = result.values.flatMap(ratings => ratings)\n",
    "\n",
    "val groundTruth: RDD[Rating] = /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8150.precisionAt.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16816.0 failed 1 times, most recent failure: Lost task 7.0 in stage 16816.0 (TID 21985, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:42)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:47)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.precisionAt(RankingMetrics.scala:79)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-d4d3f1b46bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrankingmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecisionAt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\evaluation.py\u001b[0m in \u001b[0;36mprecisionAt\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mwarning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \"\"\"\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"precisionAt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8150.precisionAt.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16816.0 failed 1 times, most recent failure: Lost task 7.0 in stage 16816.0 (TID 21985, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:42)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:47)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.precisionAt(RankingMetrics.scala:79)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "rankingmetrics.precisionAt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8150.meanAveragePrecision.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 16832.0 failed 1 times, most recent failure: Lost task 9.0 in stage 16832.0 (TID 22000, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:42)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:47)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision$lzycompute(RankingMetrics.scala:108)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision(RankingMetrics.scala:87)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-dc248d280ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrankingmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeanAveragePrecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\evaluation.py\u001b[0m in \u001b[0;36mmeanAveragePrecision\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mwarining\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meanAveragePrecision\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1.4.0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8150.meanAveragePrecision.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 16832.0 failed 1 times, most recent failure: Lost task 9.0 in stage 16832.0 (TID 22000, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:42)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:47)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision$lzycompute(RankingMetrics.scala:108)\r\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision(RankingMetrics.scala:87)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to scala.collection.Seq\r\n\tat org.apache.spark.sql.Row$class.getSeq(Row.scala:283)\r\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI$$anonfun$newRankingMetrics$1.apply(PythonMLLibAPI.scala:1070)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\r\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\r\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:43)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "print(rankingmetrics.meanAveragePrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.filter(lambda x : x[0] == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=204, rating=0.1302139087676721),\n",
       " Rating(user=0, product=288, rating=0.08125482420744135),\n",
       " Rating(user=0, product=2508, rating=0.5641149002283767),\n",
       " Rating(user=0, product=2856, rating=0.35142433988278265),\n",
       " Rating(user=0, product=2820, rating=1.6709564288988954),\n",
       " Rating(user=0, product=2808, rating=1.830578520171482),\n",
       " Rating(user=0, product=768, rating=1.2665242119013902),\n",
       " Rating(user=0, product=2844, rating=0.861640002186876),\n",
       " Rating(user=0, product=2472, rating=0.40332568305465166),\n",
       " Rating(user=0, product=2652, rating=1.096171466058772),\n",
       " Rating(user=0, product=792, rating=-0.6297734465634264),\n",
       " Rating(user=0, product=2124, rating=1.1416842288348323),\n",
       " Rating(user=0, product=132, rating=0.6341896913551128),\n",
       " Rating(user=0, product=240, rating=2.258801205518232),\n",
       " Rating(user=0, product=2893, rating=-0.01438448255242264),\n",
       " Rating(user=0, product=2089, rating=0.5896484975427381),\n",
       " Rating(user=0, product=793, rating=-0.21629310312896077),\n",
       " Rating(user=0, product=2341, rating=1.269633348320328),\n",
       " Rating(user=0, product=2773, rating=0.39540523795035676),\n",
       " Rating(user=0, product=817, rating=1.1834992743471844),\n",
       " Rating(user=0, product=253, rating=1.11657821747826),\n",
       " Rating(user=0, product=877, rating=3.605616442326278),\n",
       " Rating(user=0, product=757, rating=0.06641670945230516),\n",
       " Rating(user=0, product=2917, rating=1.6674002952833504),\n",
       " Rating(user=0, product=2510, rating=0.604066215976891),\n",
       " Rating(user=0, product=2882, rating=-0.6244523355139315),\n",
       " Rating(user=0, product=698, rating=1.690446319885468),\n",
       " Rating(user=0, product=2798, rating=1.038680586568351),\n",
       " Rating(user=0, product=2774, rating=4.481587589248868),\n",
       " Rating(user=0, product=650, rating=0.06380899438929877),\n",
       " Rating(user=0, product=2918, rating=1.2972051515271232),\n",
       " Rating(user=0, product=2738, rating=0.24596663481184378),\n",
       " Rating(user=0, product=830, rating=1.0835547441703457),\n",
       " Rating(user=0, product=746, rating=2.0237090526486146),\n",
       " Rating(user=0, product=818, rating=-1.2297959392060864),\n",
       " Rating(user=0, product=795, rating=5.739300743381179),\n",
       " Rating(user=0, product=2823, rating=1.444311168210533),\n",
       " Rating(user=0, product=2751, rating=-6.176757631672864),\n",
       " Rating(user=0, product=2811, rating=1.3323527480345563),\n",
       " Rating(user=0, product=831, rating=1.3752524050445896),\n",
       " Rating(user=0, product=2271, rating=1.5814362375269744),\n",
       " Rating(user=0, product=579, rating=0.5222630620212829),\n",
       " Rating(user=0, product=2919, rating=0.786604971067006),\n",
       " Rating(user=0, product=2175, rating=2.488628885729356),\n",
       " Rating(user=0, product=2859, rating=1.2347067971854706),\n",
       " Rating(user=0, product=843, rating=0.5104908038873723),\n",
       " Rating(user=0, product=2596, rating=0.5612694023924094),\n",
       " Rating(user=0, product=196, rating=0.9301534353923522),\n",
       " Rating(user=0, product=784, rating=2.5183920846529535),\n",
       " Rating(user=0, product=688, rating=0.461313310203435),\n",
       " Rating(user=0, product=760, rating=-0.06053525238142299),\n",
       " Rating(user=0, product=2800, rating=0.6673620884793907),\n",
       " Rating(user=0, product=2740, rating=0.05455321492450382),\n",
       " Rating(user=0, product=2728, rating=0.17892122115880582),\n",
       " Rating(user=0, product=2764, rating=5.566107364199908),\n",
       " Rating(user=0, product=17, rating=0.19653785421633208),\n",
       " Rating(user=0, product=2081, rating=0.14455401727659667),\n",
       " Rating(user=0, product=2717, rating=0.1800499512201621),\n",
       " Rating(user=0, product=2333, rating=0.9304716311467622),\n",
       " Rating(user=0, product=2849, rating=-0.48466728371875556),\n",
       " Rating(user=0, product=593, rating=-0.48888785476602026),\n",
       " Rating(user=0, product=2705, rating=-0.1251532905457884),\n",
       " Rating(user=0, product=2873, rating=0.5973595213848508),\n",
       " Rating(user=0, product=2861, rating=-0.3511987758205297),\n",
       " Rating(user=0, product=2813, rating=0.4003131460134277),\n",
       " Rating(user=0, product=737, rating=0.9412619311784365),\n",
       " Rating(user=0, product=845, rating=0.20523497006504599),\n",
       " Rating(user=0, product=2393, rating=1.7229469962249482),\n",
       " Rating(user=0, product=749, rating=1.044963542991857),\n",
       " Rating(user=0, product=138, rating=1.5575738992065844),\n",
       " Rating(user=0, product=2850, rating=0.5739047540389306),\n",
       " Rating(user=0, product=330, rating=-0.21101418708922726),\n",
       " Rating(user=0, product=2778, rating=0.36505040572019176),\n",
       " Rating(user=0, product=858, rating=0.9812600864822238),\n",
       " Rating(user=0, product=2730, rating=4.073522523357298),\n",
       " Rating(user=0, product=2826, rating=-0.27309744437746497),\n",
       " Rating(user=0, product=510, rating=-0.10347871927678787),\n",
       " Rating(user=0, product=798, rating=1.5382061426205276),\n",
       " Rating(user=0, product=750, rating=0.5020245649508261),\n",
       " Rating(user=0, product=714, rating=1.4558275968176169),\n",
       " Rating(user=0, product=2766, rating=-0.5032444089156662),\n",
       " Rating(user=0, product=703, rating=1.4752830715070466),\n",
       " Rating(user=0, product=775, rating=1.536291966685642),\n",
       " Rating(user=0, product=427, rating=0.1286049013021402),\n",
       " Rating(user=0, product=2911, rating=-0.29426895258977037),\n",
       " Rating(user=0, product=847, rating=0.5820571758625088),\n",
       " Rating(user=0, product=823, rating=0.7496045325717482),\n",
       " Rating(user=0, product=2071, rating=0.8594105279958679),\n",
       " Rating(user=0, product=2755, rating=3.4342415300414313),\n",
       " Rating(user=0, product=2815, rating=0.33054796957932986),\n",
       " Rating(user=0, product=763, rating=2.2730195668089586),\n",
       " Rating(user=0, product=871, rating=0.8526492448551967),\n",
       " Rating(user=0, product=691, rating=0.8573929088352905),\n",
       " Rating(user=0, product=728, rating=0.807503730163436),\n",
       " Rating(user=0, product=812, rating=-0.08541414888221954),\n",
       " Rating(user=0, product=2756, rating=1.2425252654209658),\n",
       " Rating(user=0, product=56, rating=0.7942697595267121),\n",
       " Rating(user=0, product=740, rating=-0.4207904202071733),\n",
       " Rating(user=0, product=2792, rating=0.9640665445773031),\n",
       " Rating(user=0, product=8, rating=1.07698673708897),\n",
       " Rating(user=0, product=2888, rating=1.680526110808632),\n",
       " Rating(user=0, product=584, rating=-0.7644035168703747),\n",
       " Rating(user=0, product=380, rating=1.0494591446476722),\n",
       " Rating(user=0, product=596, rating=1.8789798496679326),\n",
       " Rating(user=0, product=2612, rating=1.8092570701745982),\n",
       " Rating(user=0, product=776, rating=1.7798881482871982),\n",
       " Rating(user=0, product=729, rating=0.17503849072686517),\n",
       " Rating(user=0, product=417, rating=2.320986575132748),\n",
       " Rating(user=0, product=789, rating=2.9157612980395022),\n",
       " Rating(user=0, product=765, rating=0.6051603941404449),\n",
       " Rating(user=0, product=2505, rating=2.214015237707722),\n",
       " Rating(user=0, product=2806, rating=1.3027635643741373),\n",
       " Rating(user=0, product=58, rating=1.4905875514066995),\n",
       " Rating(user=0, product=2818, rating=0.0850000240002422),\n",
       " Rating(user=0, product=766, rating=3.616338930873603),\n",
       " Rating(user=0, product=814, rating=-0.24193237836318882),\n",
       " Rating(user=0, product=2782, rating=1.0761655348412713),\n",
       " Rating(user=0, product=730, rating=-1.898329852134431),\n",
       " Rating(user=0, product=2890, rating=0.5634295358090231),\n",
       " Rating(user=0, product=719, rating=0.5770689682859733),\n",
       " Rating(user=0, product=2747, rating=0.8498296044070646),\n",
       " Rating(user=0, product=83, rating=1.4446357277110735),\n",
       " Rating(user=0, product=779, rating=1.3874647517902572),\n",
       " Rating(user=0, product=371, rating=0.1983989761177849),\n",
       " Rating(user=0, product=2735, rating=0.31640342543131084),\n",
       " Rating(user=0, product=767, rating=1.5174950160452045),\n",
       " Rating(user=0, product=2075, rating=-0.3306930937760848),\n",
       " Rating(user=0, product=875, rating=1.0811040972029922),\n",
       " Rating(user=0, product=2099, rating=-0.9904766991782497),\n",
       " Rating(user=0, product=839, rating=0.7295694282492668),\n",
       " Rating(user=0, product=2771, rating=0.9652093100644841)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions.filter(lambda x : x[0] == 0).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "sparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
